defaults:
  - hydra: default
  - _self_
  - model: cprnn

save_dir: "/path/to/logs_wandb"
seed: 8

logger:
  _target_: lightning.pytorch.loggers.WandbLogger
  entity: entity
  project: project
  name: ${now:%Y-%m-%d}_${now:%H-%M-%S}
  save_dir: ${save_dir}
  offline: False
  tags: null

params_dataset:
  vocab_size: &vocab_size
    107 

datamodule:
  _target_: data_shakespeare.ShakespeareDataModule
  tokenizer:
    #_target_: transformers.AutoTokenizer.from_pretrained
    #pretrained_model_name_or_path: "gpt2"
    _target_: charactertokenizer.CharacterTokenizer
    characters: string.printable
    model_max_length: 64
  #vocab_size: 50257 # known value for gpt-2 tokenizer, hacky but it works 
  vocab_size: *vocab_size
  input_seq_len: 64 
  batch_size: 64 

trainer:
  max_epochs: 1000 
  enable_progress_bar: True
  log_every_n_steps: 1
  limit_train_batches: 1.0
  accelerator: "cpu"

model: ${model}

callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val_loss
    patience: 200
    mode: min
    verbose: True
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val_loss
    mode: min
    save_top_k: 1
    filename: "{epoch:02d}-{val_loss:.2f}"
    dirpath: ${save_dir}
    verbose: True

task :
  _target_: task.ShakespeareTask
  model: ${model}
  lr: 1e-4
